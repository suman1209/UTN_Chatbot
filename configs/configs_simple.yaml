task: "interactive" # 'train', 'evaluate', 'inference', 'inference_full', 'interactive'
data_configs:
  sys_role: "You are a helpful assistant working in a university called UTN(University of Technology Nuremberg), \
             it is a new university and offers master courses, your job is to answer questions \
             about the university, master's program, life in germany..\
             You are a helpful assistant for the students who \
             want to learn about UTN's master coursers and or PHD courses"
  data_path: "./datasets/LLM Project - UTN Site Data.tsv"
  shuffle: True
  batch_size: 2
  rephrase: 0

model_configs:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  tokenizer_name: "Qwen/Qwen2.5-0.5B-Instruct"
  checkpoint: './docs_and_results/checkpoints/checkpoint_final'
  save_checkpoint: False
  device: "cuda" # either "cpu" or "cuda"
  device_map: { "": 0 }
  cache_dir: ./cache

training_configs:
  output_dir: ./docs_and_results/checkpoints
  plot: True
  epochs: 10000
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  logging_steps: 0.01 # Log every X updates steps
  logging_strategy: steps
  save_steps: 0.1
  max_steps: -1
  warmup_ratio: 0.1
  group_by_length: True
  lr_scheduler_type: cosine
  do_eval: True
  eval_strategy: steps
  eval_steps: 0.1
  save_total_limit: 1
  load_best_model_at_end: True
  metric_for_best_model: eval_loss
  optim:
    name: adamw_torch
    lr: 0.001
    weight_decay: 0.0005
    clip_grad: null

task_configs:
  debug: True # if True will display a lot of intermediate information for debugging purposes
  log_expt: True # whether to log the experiment online or not
  vocab_size: null
